import { create } from 'zustand';
import { persist } from 'zustand/middleware';
import type { TabId, ModelConfig } from '../types';

/**
 * Wikipedia links for ML terms - opens in new tab
 */
export const ML_WIKI_LINKS = {
  correlation: 'https://en.wikipedia.org/wiki/Pearson_correlation_coefficient',
  loss: 'https://en.wikipedia.org/wiki/Loss_function',
  r2: 'https://en.wikipedia.org/wiki/Coefficient_of_determination',
  bias: 'https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff',
  normalization: 'https://en.wikipedia.org/wiki/Feature_scaling',
  zScore: 'https://en.wikipedia.org/wiki/Standard_score',
  minMax: 'https://en.wikipedia.org/wiki/Feature_scaling#Rescaling_(min-max_normalization)',
  neuralNetwork: 'https://en.wikipedia.org/wiki/Artificial_neural_network',
  hiddenLayer: 'https://en.wikipedia.org/wiki/Multilayer_perceptron',
  activation: 'https://en.wikipedia.org/wiki/Activation_function',
  epoch: 'https://en.wikipedia.org/wiki/Epoch_(computing)',
  batchSize: 'https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Iterative_method',
  learningRate: 'https://en.wikipedia.org/wiki/Learning_rate',
  overfitting: 'https://en.wikipedia.org/wiki/Overfitting',
  underfitting: 'https://en.wikipedia.org/wiki/Underfitting',
  gaussian: 'https://en.wikipedia.org/wiki/Normal_distribution',
  pca: 'https://en.wikipedia.org/wiki/Principal_component_analysis',
  residual: 'https://en.wikipedia.org/wiki/Residual_(statistics)',
  mse: 'https://en.wikipedia.org/wiki/Mean_squared_error',
  rmse: 'https://en.wikipedia.org/wiki/Root-mean-square_deviation',
  mae: 'https://en.wikipedia.org/wiki/Mean_absolute_error',
  validation: 'https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets',
} as const;

/**
 * Tutorial step definition
 */
export interface TutorialStep {
  id: string;
  title: string;
  content: string; // Markdown content with wiki links
  tab?: TabId; // Which tab to navigate to
  highlightSelector?: string; // CSS selector for element to highlight
  highlightPadding?: number; // Padding around highlight (default 8)
  position?: 'top' | 'bottom' | 'left' | 'right' | 'center'; // Tooltip position
  // Auto-configuration for this step
  autoConfig?: {
    inputFeatures?: string[]; // Feature IDs to select
    targetFeature?: string;
    modelConfig?: Partial<ModelConfig>;
    createTransforms?: Array<{
      sourceFeatureId: string;
      transform: string;
      params?: Record<string, number>;
    }>;
  };
  // Action requirements
  requiresAction?: boolean; // User must perform action, Next is disabled
  autoAdvanceOnTrainingStart?: boolean; // Auto-advance when training starts
  // Wait conditions
  waitForTraining?: boolean; // Wait for training to complete before enabling Next
  waitForSelector?: string; // Wait for element to appear
  // Action hint
  actionLabel?: string; // Label for action hint (e.g., "Click the train button")
}

interface TutorialState {
  // Tutorial active state
  isActive: boolean;
  hasSeenTutorial: boolean;
  currentStepIndex: number;

  // Training wait state
  isWaitingForTraining: boolean;

  // UI state
  showWelcome: boolean;
}

interface TutorialActions {
  // Core actions
  startTutorial: () => void;
  endTutorial: () => void;
  skipTutorial: () => void;

  // Navigation
  nextStep: () => void;
  prevStep: () => void;
  goToStep: (index: number) => void;

  // Training wait
  setWaitingForTraining: (waiting: boolean) => void;

  // Welcome dialog
  showWelcomeDialog: () => void;
  hideWelcomeDialog: () => void;

  // Getters
  getCurrentStep: () => TutorialStep | null;
  getTotalSteps: () => number;
  getProgress: () => number; // 0-100
}

export type TutorialStore = TutorialState & TutorialActions;

/**
 * Tutorial steps following STORYTELLING.md
 */
export const TUTORIAL_STEPS: TutorialStep[] = [
  // Step 0: Welcome / Problem Introduction
  {
    id: 'welcome',
    title: 'Welcome to Airfoil Noise Prediction',
    content: `This interactive tutorial will teach you **machine learning fundamentals** by solving a real engineering problem.

**The Problem:** Predict the noise (sound pressure level) generated by aircraft wing sections called airfoils.

The **NACA 0012** airfoil is a symmetric wing profile used in aerodynamics research. When air flows over it at different speeds and angles, it creates noise. NASA collected this dataset to study this phenomenon.

We have **5 input features**:
- **Frequency** (Hz) - Sound frequency being measured
- **Angle of Attack** (degrees) - Wing tilt angle
- **Chord Length** (m) - Wing section width
- **Free Stream Velocity** (m/s) - Air speed
- **Suction Side Displacement Thickness** (m) - Boundary layer size

And **1 target** to predict:
- **Sound Pressure Level** (dB) - The noise intensity

Let's solve this with a [neural network](${ML_WIKI_LINKS.neuralNetwork})!`,
    position: 'center',
  },

  // Step 1: Look at Data - Correlation
  {
    id: 'explore-correlation',
    title: 'Step 1: Explore the Data',
    content: `Before training any model, we need to **understand our data**.

This **correlation heatmap** shows the [Pearson correlation](${ML_WIKI_LINKS.correlation}) between all features.

Values range from **-1** (inverse relationship) to **+1** (direct relationship). Zero means no linear relationship.

Notice that **Frequency** has the highest anti-correlation with Sound Pressure Level (**-0.39**). This means higher frequencies tend to produce lower noise levels.

Maybe training with just frequency could work? Let's find out!`,
    tab: 'explore',
    highlightSelector: '[data-tutorial="correlation-heatmap"]',
    position: 'right',
  },

  // Step 2: Train First Model - Configure
  {
    id: 'train-setup-bad',
    title: 'Step 2: Train a Simple Model',
    content: `Let's train our first [neural network](${ML_WIKI_LINKS.neuralNetwork})!

We'll start with a **simple architecture**:
- 1 [hidden layer](${ML_WIKI_LINKS.hiddenLayer}) with just 4 neurons
- Using all 5 raw features as inputs

The network will learn to map inputs to the target sound pressure level.

Click **Start Training** to begin!`,
    tab: 'train',
    highlightSelector: '[data-tutorial="train-button"]',
    position: 'left',
    autoConfig: {
      inputFeatures: ['frequency', 'angleOfAttack', 'chordLength', 'freeStreamVelocity', 'suctionSideDisplacementThickness'],
      targetFeature: 'soundPressureLevel',
      modelConfig: {
        hiddenLayers: [{ neurons: 4, activation: 'relu' }],
        epochs: 100,
        learningRate: 0.001,
      },
    },
    requiresAction: true,
    autoAdvanceOnTrainingStart: true,
    actionLabel: 'Click the highlighted "Start Training" button',
  },

  // Step 3: Observe Training Progress
  {
    id: 'observe-training-bad',
    title: 'Step 3: Watch Training Progress',
    content: `Watch the **loss chart** as training progresses!

The [loss](${ML_WIKI_LINKS.loss}) measures how wrong the model's predictions are. Lower is better.

- **Blue line**: Training loss (how well it fits training data)
- **Orange line**: Validation loss (how well it generalizes to unseen data)

Ideally, both should decrease together. If validation loss increases while training loss decreases, that's [overfitting](${ML_WIKI_LINKS.overfitting}).

Let's see how this simple model performs...`,
    tab: 'train',
    highlightSelector: '[data-tutorial="loss-chart"]',
    position: 'left',
    waitForTraining: true, // Wait for training to complete
  },

  // Step 4: Results are Bad
  {
    id: 'results-bad',
    title: 'Step 4: Analyzing Poor Results',
    content: `The model performed **terribly**!

Look at the [R-squared](${ML_WIKI_LINKS.r2}) value - it might even be **negative**, meaning our model is worse than just predicting the average value every time!

The dots in the prediction plot are scattered everywhere - far from the diagonal line that would indicate perfect predictions.

This is called **[underfitting](${ML_WIKI_LINKS.underfitting})** - the model can't capture patterns in the data.

**Why?** Let's look at our feature statistics to understand...`,
    tab: 'train',
    highlightSelector: '[data-tutorial="prediction-scatter"]',
    position: 'left',
  },

  // Step 5: Look at Feature Ranges
  {
    id: 'feature-ranges',
    title: 'Step 5: The Problem with Raw Features',
    content: `Look at the **feature ranges** - they're wildly different!

- **Frequency**: 200 - 20,000 Hz (huge range!)
- **Chord Length**: 0.025 - 0.305 m (tiny numbers)
- **Velocity**: 31 - 72 m/s

When features have such different scales, the neural network struggles. Features with larger values dominate the learning process.

The solution? **[Feature Normalization](${ML_WIKI_LINKS.normalization})**!

Let's apply [Z-score normalization](${ML_WIKI_LINKS.zScore}) (standardization) to all features. This transforms each feature to have mean=0 and standard deviation=1.`,
    tab: 'explore',
    highlightSelector: '[data-tutorial="feature-stats"]',
    position: 'right',
  },

  // Step 6: Create Z-Score Transforms
  {
    id: 'create-transforms-z',
    title: 'Step 6: Normalize Features',
    content: `Now we'll create **Z-score normalized** versions of all our features.

Click **Add Transform** to open the transform dialog. We'll automatically create Z-score transforms for all input features.

[Z-score normalization](${ML_WIKI_LINKS.zScore}) uses the formula:
**z = (x - mean) / std**

This centers the data around 0 and scales by standard deviation, making all features comparable.`,
    tab: 'explore',
    highlightSelector: '[data-tutorial="add-transform-button"]',
    position: 'right',
    autoConfig: {
      createTransforms: [
        { sourceFeatureId: 'frequency', transform: 'zscore' },
        { sourceFeatureId: 'angleOfAttack', transform: 'zscore' },
        { sourceFeatureId: 'chordLength', transform: 'zscore' },
        { sourceFeatureId: 'freeStreamVelocity', transform: 'zscore' },
        { sourceFeatureId: 'suctionSideDisplacementThickness', transform: 'zscore' },
        { sourceFeatureId: 'soundPressureLevel', transform: 'zscore' },
      ],
    },
  },

  // Step 7: Train with Normalized Features
  {
    id: 'train-normalized',
    title: 'Step 7: Train with Normalized Features',
    content: `Now let's train again using our **normalized features**!

We've selected all the Z-score transformed features as inputs and the normalized target.

Same simple architecture - 1 hidden layer with 4 neurons. Let's see if normalization helps!

Click **Start Training** to begin.`,
    tab: 'train',
    highlightSelector: '[data-tutorial="train-button"]',
    position: 'left',
    autoConfig: {
      inputFeatures: [
        'frequency_zscore_1',
        'angleOfAttack_zscore_1',
        'chordLength_zscore_1',
        'freeStreamVelocity_zscore_1',
        'suctionSideDisplacementThickness_zscore_1',
      ],
      targetFeature: 'soundPressureLevel_zscore_1',
      modelConfig: {
        hiddenLayers: [{ neurons: 4, activation: 'relu' }],
        epochs: 100,
        learningRate: 0.001,
      },
    },
    requiresAction: true,
    autoAdvanceOnTrainingStart: true,
    actionLabel: 'Click the highlighted "Start Training" button',
  },

  // Step 8: Observe Training Progress (Normalized)
  {
    id: 'observe-training-normalized',
    title: 'Step 8: Watch the Improvement',
    content: `Watch how the loss behaves with **normalized features**!

You should see:
- The loss starts **much lower** than before
- Both curves decrease more smoothly
- Training is more stable overall

[Feature normalization](${ML_WIKI_LINKS.normalization}) makes gradient descent much more efficient by ensuring all features contribute equally to learning.`,
    tab: 'train',
    highlightSelector: '[data-tutorial="loss-chart"]',
    position: 'left',
    waitForTraining: true,
  },

  // Step 9: Better Results
  {
    id: 'results-better',
    title: 'Step 9: Much Better!',
    content: `**Huge improvement!**

The [R-squared](${ML_WIKI_LINKS.r2}) is now above **0.6** - the model explains 60% of the variance in sound pressure levels.

The dots are now closer to the diagonal!

But notice: both **training loss** and **validation loss** are still relatively high. This is a sign of **[high bias](${ML_WIKI_LINKS.bias})** - the model is too simple to capture the full complexity.

Solution? **Add more capacity** - let's try a deeper network!`,
    tab: 'train',
    highlightSelector: '[data-tutorial="prediction-scatter"]',
    position: 'left',
  },

  // Step 10: Train Deeper Network
  {
    id: 'train-deeper',
    title: 'Step 10: A Deeper Network',
    content: `Let's add a **second hidden layer** to give the network more capacity to learn complex patterns.

Architecture:
- Hidden Layer 1: 8 neurons
- Hidden Layer 2: 4 neurons

More layers allow the network to learn hierarchical representations - combining simple features into complex patterns.

Click **Start Training**!`,
    tab: 'train',
    highlightSelector: '[data-tutorial="train-button"]',
    position: 'left',
    autoConfig: {
      inputFeatures: [
        'frequency_zscore_1',
        'angleOfAttack_zscore_1',
        'chordLength_zscore_1',
        'freeStreamVelocity_zscore_1',
        'suctionSideDisplacementThickness_zscore_1',
      ],
      targetFeature: 'soundPressureLevel_zscore_1',
      modelConfig: {
        hiddenLayers: [
          { neurons: 8, activation: 'relu' },
          { neurons: 4, activation: 'relu' },
        ],
        epochs: 100,
        learningRate: 0.001,
      },
    },
    requiresAction: true,
    autoAdvanceOnTrainingStart: true,
    actionLabel: 'Click the highlighted "Start Training" button',
  },

  // Step 11: Observe Training Progress (Deeper)
  {
    id: 'observe-training-deeper',
    title: 'Step 11: Watch Deeper Network Train',
    content: `Watch how the **deeper network** trains!

With more layers, the network can learn hierarchical representations - combining simple features into complex patterns.

Notice how the loss continues to decrease even after the simpler model would have plateaued. More capacity means more room to learn!`,
    tab: 'train',
    highlightSelector: '[data-tutorial="loss-chart"]',
    position: 'left',
    waitForTraining: true,
  },

  // Step 12: Even Better
  {
    id: 'results-deeper',
    title: 'Step 12: Even Better Results!',
    content: `Excellent! [R-squared](${ML_WIKI_LINKS.r2}) is now around **0.7**!

The deeper network can learn more complex patterns in the data. But we can do even better...

Let's go back to the data and look at the **distributions** of each feature. Maybe there's something special about how they're shaped?`,
    tab: 'train',
    highlightSelector: '[data-tutorial="prediction-scatter"]',
    position: 'left',
  },

  // Step 13: Look at Distributions
  {
    id: 'distributions',
    title: 'Step 13: Feature Distributions',
    content: `Look at the **distribution histograms** for each feature.

Notice that different features have very different shapes:
- **Sound Pressure Level**: Roughly [Gaussian](${ML_WIKI_LINKS.gaussian}) (bell-shaped) - Z-score works well here
- **Angle of Attack, Chord Length, Velocity**: Not Gaussian - might benefit from [min-max scaling](${ML_WIKI_LINKS.minMax})
- **Frequency**: Very skewed (non-Gaussian) - a **log transform** might help
- **Displacement Thickness**: Unusual shape - perhaps a power transform like **x^0.25**

Let's create specialized transforms for each feature based on their distributions!`,
    tab: 'explore',
    highlightSelector: '[data-tutorial="distribution-chart"]',
    position: 'right',
  },

  // Step 14: Create Specialized Transforms
  {
    id: 'create-transforms-custom',
    title: 'Step 14: Custom Transforms',
    content: `Now we'll create **specialized transforms** for each feature:

- **Frequency**: Log transform (handles the skewed distribution)
- **Angle of Attack, Chord Length, Velocity**: Min-max scaling
- **Displacement Thickness**: Power transform (x^0.25)
- **Sound Pressure Level**: Keep Z-score (it's already Gaussian)

These domain-specific transforms can significantly improve model performance by making the data more "learnable"!`,
    tab: 'explore',
    highlightSelector: '[data-tutorial="feature-engineering"]',
    position: 'right',
    autoConfig: {
      createTransforms: [
        { sourceFeatureId: 'frequency', transform: 'log' },
        { sourceFeatureId: 'angleOfAttack', transform: 'minmax' },
        { sourceFeatureId: 'chordLength', transform: 'minmax' },
        { sourceFeatureId: 'freeStreamVelocity', transform: 'minmax' },
        { sourceFeatureId: 'suctionSideDisplacementThickness', transform: 'power', params: { power: 0.25 } },
      ],
    },
  },

  // Step 15: Train with Custom Transforms
  {
    id: 'train-custom',
    title: 'Step 15: Train with Custom Transforms',
    content: `Now let's train with our **specialized transforms**!

We'll use a simpler architecture - just **1 hidden layer** with 8 neurons - to show that good feature engineering can be more important than complex networks.

Click **Start Training**!`,
    tab: 'train',
    highlightSelector: '[data-tutorial="train-button"]',
    position: 'left',
    autoConfig: {
      inputFeatures: [
        'frequency_log_1',
        'angleOfAttack_minmax_1',
        'chordLength_minmax_1',
        'freeStreamVelocity_minmax_1',
        'suctionSideDisplacementThickness_power_1',
      ],
      targetFeature: 'soundPressureLevel_zscore_1',
      modelConfig: {
        hiddenLayers: [{ neurons: 8, activation: 'relu' }],
        epochs: 100,
        learningRate: 0.001,
      },
    },
    requiresAction: true,
    autoAdvanceOnTrainingStart: true,
    actionLabel: 'Click the highlighted "Start Training" button',
  },

  // Step 16: Observe Training Progress (Custom)
  {
    id: 'observe-training-custom',
    title: 'Step 16: Watch Custom Transforms Train',
    content: `Watch how the model performs with **custom transforms**!

Even with a simpler 1-layer architecture, good feature engineering can achieve excellent results.

The right transforms make the relationships between features and target more **linear and learnable** for the network.`,
    tab: 'train',
    highlightSelector: '[data-tutorial="loss-chart"]',
    position: 'left',
    waitForTraining: true,
  },

  // Step 17: Amazing Results
  {
    id: 'results-custom',
    title: 'Step 17: Outstanding Results!',
    content: `**Wow!** Look at those results!

With the right transforms, even a **single-layer network** achieves better performance than a deeper network with simple Z-score normalization!

This demonstrates a key insight in machine learning:

> **Feature engineering is often more important than model complexity.**

Good data preparation can make the difference between a mediocre and an excellent model.`,
    tab: 'train',
    highlightSelector: '[data-tutorial="metrics-panel"]',
    position: 'left',
  },

  // Step 18: Conclusion
  {
    id: 'conclusion',
    title: 'Congratulations!',
    content: `You've learned the fundamentals of machine learning!

**Key Takeaways:**
1. Always **explore your data** before training
2. [Feature scaling/normalization](${ML_WIKI_LINKS.normalization}) is essential
3. Match transforms to feature **distributions**
4. More layers isn't always better - try good features first
5. Use [R-squared](${ML_WIKI_LINKS.r2}), [loss curves](${ML_WIKI_LINKS.loss}), and prediction plots to evaluate

**What's Next?**
- Try [PCA](${ML_WIKI_LINKS.pca}) to reduce dimensionality
- Experiment with different architectures
- Use the **Predict** tab to make predictions
- Play with [learning rate](${ML_WIKI_LINKS.learningRate}) and [batch size](${ML_WIKI_LINKS.batchSize})

Have fun exploring!`,
    position: 'center',
  },
];

const initialState: TutorialState = {
  isActive: false,
  hasSeenTutorial: false,
  currentStepIndex: 0,
  isWaitingForTraining: false,
  showWelcome: false,
};

export const useTutorialStore = create<TutorialStore>()(
  persist(
    (set, get) => ({
      ...initialState,

      startTutorial: () => {
        set({
          isActive: true,
          currentStepIndex: 0,
          showWelcome: false,
          isWaitingForTraining: false,
        });
      },

      endTutorial: () => {
        set({
          isActive: false,
          hasSeenTutorial: true,
          currentStepIndex: 0,
          isWaitingForTraining: false,
        });
      },

      skipTutorial: () => {
        set({
          isActive: false,
          hasSeenTutorial: true,
          showWelcome: false,
          isWaitingForTraining: false,
        });
      },

      nextStep: () => {
        const { currentStepIndex, isWaitingForTraining } = get();
        if (isWaitingForTraining) return; // Can't advance while waiting

        const nextIndex = currentStepIndex + 1;
        if (nextIndex >= TUTORIAL_STEPS.length) {
          get().endTutorial();
        } else {
          set({ currentStepIndex: nextIndex });
        }
      },

      prevStep: () => {
        const { currentStepIndex, isWaitingForTraining } = get();
        if (isWaitingForTraining) return;

        if (currentStepIndex > 0) {
          set({ currentStepIndex: currentStepIndex - 1 });
        }
      },

      goToStep: (index: number) => {
        if (index >= 0 && index < TUTORIAL_STEPS.length) {
          set({ currentStepIndex: index, isWaitingForTraining: false });
        }
      },

      setWaitingForTraining: (waiting: boolean) => {
        set({ isWaitingForTraining: waiting });
      },

      showWelcomeDialog: () => {
        set({ showWelcome: true });
      },

      hideWelcomeDialog: () => {
        set({ showWelcome: false });
      },

      getCurrentStep: () => {
        const { currentStepIndex, isActive } = get();
        if (!isActive) return null;
        return TUTORIAL_STEPS[currentStepIndex] ?? null;
      },

      getTotalSteps: () => TUTORIAL_STEPS.length,

      getProgress: () => {
        const { currentStepIndex } = get();
        return ((currentStepIndex + 1) / TUTORIAL_STEPS.length) * 100;
      },
    }),
    {
      name: 'airfoil-tutorial',
      partialize: (state) => ({
        hasSeenTutorial: state.hasSeenTutorial,
      }),
    }
  )
);
